{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ActlrnDtree': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a0e1ce46a9a75413741907f25713a87c366a68c3c5234538965b0997e1400dd8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\ncpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "nspl_input_ts = torch.load(\"/Users/palash.sethi/Downloads/Projects/Refinery Optimisation/NSPL/data/scaled_dataframes/nspl_ann/combined_ip.pt\")\n",
    "nspl_input_ts = nspl_input_ts.to(device)\n",
    "nspl_output_ts = torch.load(\"/Users/palash.sethi/Downloads/Projects/Refinery Optimisation/NSPL/data/scaled_dataframes/nspl_ann/combined_op.pt\")\n",
    "nspl_output_ts = nspl_output_ts.to(device)\n",
    "\n",
    "class ReactorSimulatorDataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, x_path, y_path):\n",
    "        'Initialization'\n",
    "        self.x = torch.load(x_path)\n",
    "        self.y = torch.load(y_path)\n",
    "        # self.count = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "dataset = ReactorSimulatorDataset( \\\n",
    "        x_path = \"/Users/palash.sethi/Downloads/Projects/Refinery Optimisation/NSPL/data/scaled_dataframes/nspl_ann/combined_ip.pt\",\n",
    "        y_path = \"/Users/palash.sethi/Downloads/Projects/Refinery Optimisation/NSPL/data/scaled_dataframes/nspl_ann/combined_op.pt\"\n",
    "    )\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "training_generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size,num_workers = 0 ,\n",
    "                                           sampler=train_sampler)\n",
    "validation_generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers = 0,\n",
    "                                                sampler=valid_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss at epoch  0  is  1.0164624020444746\n",
      "Training MAPE loss at epoch  0  is  43.59238259352722\n",
      "('Validation loss at epoch ', 0, ' is ', 0.6384130668042387)\n",
      "('Validation MAPE loss at epoch ', 0, ' is ', 40.534754174929134)\n",
      "('Average Time Taken for epoch ', 0, ' is ', 151550.0)\n",
      "Training loss at epoch  10  is  0.3927425836744483\n",
      "Training MAPE loss at epoch  10  is  36.17419031342816\n",
      "('Validation loss at epoch ', 10, ' is ', 0.35643870872527944)\n",
      "('Validation MAPE loss at epoch ', 10, ' is ', 35.488023623332104)\n",
      "('Average Time Taken for epoch ', 10, ' is ', 255593.9090909091)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-81bf1f9e73da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mepoch_train_mape_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmape_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ActlrnDtree/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ActlrnDtree/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ActlrnDtree/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def avg_loss(loss, size):\n",
    "    return float(sum(loss)/size)\n",
    "\n",
    "def MAPELoss(output, target):\n",
    "    # print(\"lol: \", )\n",
    "    # print(\"lol: \", output.shape)\n",
    "    return torch.mean(torch.abs((target - output)) / (torch.abs(target) + torch.abs(output)))* 100\n",
    "\n",
    "\n",
    "class Net2(torch.nn.Module):  # training only bottom stream\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(34,80)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(80,29)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net2()\n",
    "net.double()\n",
    "net.to(device)\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_mape_loss_list = []\n",
    "val_mape_loss_list = []\n",
    "time_list = []\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "epoch = 1000\n",
    "for j in range(epoch):\n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_train_mape_loss = []\n",
    "    epoch_val_mape_loss = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for x_batch, y_batch in training_generator:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        nn_output_batch = net(x_batch)\n",
    "        loss = torch.mean(nn.MSELoss(reduction='none')(nn_output_batch, y_batch))\n",
    "        epoch_train_loss.append(loss.item())\n",
    "        mape_loss = MAPELoss(nn_output_batch, y_batch)\n",
    "        epoch_train_mape_loss.append(mape_loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    train_loss_list.append(avg_loss(epoch_train_loss, len(epoch_train_loss)))\n",
    "    train_mape_loss_list.append(avg_loss(epoch_train_mape_loss, len(epoch_train_mape_loss)))\n",
    "    time_list.append((end_time - start_time).microseconds)\n",
    "\n",
    "    if not j % 10:\n",
    "        for x_batch, y_batch in validation_generator:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            nn_output_batch = net(x_batch)\n",
    "            loss = torch.mean(nn.MSELoss(reduction='none')(nn_output_batch, y_batch))\n",
    "            epoch_val_loss.append(loss.item())\n",
    "            epoch_val_mape_loss.append(MAPELoss(nn_output_batch, y_batch).item())\n",
    "        val_loss_list.append(avg_loss(epoch_val_loss, len(epoch_val_loss)))\n",
    "        val_mape_loss_list.append(avg_loss(epoch_val_mape_loss, len(epoch_val_mape_loss)))\n",
    "        print(\"Training loss at epoch \", j,' is ', train_loss_list[-1])\n",
    "        print(\"Training MAPE loss at epoch \", j,' is ', train_mape_loss_list[-1])\n",
    "        print((\"Validation loss at epoch \", j,' is ', val_loss_list[-1]))\n",
    "        print((\"Validation MAPE loss at epoch \", j,' is ', val_mape_loss_list[-1]))\n",
    "        print((\"Average Time Taken for epoch \", j,' is ', sum(time_list)/ len(time_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}